{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_ids = [4,5,6,7]  # Example: using 4 out of 7 GPUs (can customize which ones)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(i) for i in gpu_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 3\n",
      "Using GPUs with IDs: [4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"Available GPUs: {len(gpus)}\")\n",
    "print(f\"Using GPUs with IDs: {gpu_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2')\n",
      "Number of devices: 3\n"
     ]
    }
   ],
   "source": [
    "# Configure multi-GPU strategy\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(f'Number of devices: {strategy.num_replicas_in_sync}')\n",
    "\n",
    "# Set mixed precision policy for A100s\n",
    "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.set_global_policy(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2 * strategy.num_replicas_in_sync\n",
    "IMG_SIZE = (256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_images(root_dir, target_size=(512, 512), test_size=0.2, val_size=0.1, batch_size=8):\n",
    "    \"\"\"\n",
    "    Load and preprocess images from the specified directory structure.\n",
    "    \n",
    "    Args:\n",
    "        root_dir (str): Path to the root directory containing fat percentage folders\n",
    "        target_size (tuple): Target size for resizing images (height, width)\n",
    "        test_size (float): Proportion of data for testing\n",
    "        val_size (float): Proportion of training data for validation\n",
    "        batch_size (int): Batch size for data generators\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (train_gen, val_gen, test_gen, class_indices)\n",
    "    \"\"\"\n",
    "    # Initialize lists to store images and labels\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_names = []\n",
    "    class_indices = {}\n",
    "    \n",
    "    # Walk through directory structure\n",
    "    for fat_dir in sorted(os.listdir(root_dir)):\n",
    "        fat_path = os.path.join(root_dir, fat_dir)\n",
    "        if not os.path.isdir(fat_path):\n",
    "            continue\n",
    "            \n",
    "        for conc_dir in sorted(os.listdir(fat_path)):\n",
    "            conc_path = os.path.join(fat_path, conc_dir)\n",
    "            if not os.path.isdir(conc_path):\n",
    "                continue\n",
    "                \n",
    "            # Handle no-adulteration case (0%)\n",
    "            if conc_dir == '0':\n",
    "                for img_file in os.listdir(conc_path):\n",
    "                    if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        img_path = os.path.join(conc_path, img_file)\n",
    "                        label = 'no_adulteration'\n",
    "                        \n",
    "                        # Add to class indices if not present\n",
    "                        if label not in class_indices:\n",
    "                            class_indices[label] = len(class_indices)\n",
    "                            class_names.append(label)\n",
    "                            \n",
    "                        images.append(img_path)\n",
    "                        labels.append(class_indices[label])\n",
    "            else:\n",
    "                # Handle adulteration cases (5%, 10%, 15%)\n",
    "                for adulterant_dir in os.listdir(conc_path):\n",
    "                    adulterant_path = os.path.join(conc_path, adulterant_dir)\n",
    "                    if not os.path.isdir(adulterant_path):\n",
    "                        continue\n",
    "                        \n",
    "                    label = f\"{adulterant_dir}_{conc_dir}%\"\n",
    "                    \n",
    "                    # Add to class indices if not present\n",
    "                    if label not in class_indices:\n",
    "                        class_indices[label] = len(class_indices)\n",
    "                        class_names.append(label)\n",
    "                        \n",
    "                    for img_file in os.listdir(adulterant_path):\n",
    "                        if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                            img_path = os.path.join(adulterant_path, img_file)\n",
    "                            images.append(img_path)\n",
    "                            labels.append(class_indices[label])\n",
    "    \n",
    "    # Split into train, validation, and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        images, labels, test_size=test_size, stratify=labels, random_state=42\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=val_size, stratify=y_train, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create data generators with preprocessing\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    def create_generator(data_gen, X, y, shuffle=False):\n",
    "        df = pd.DataFrame({'filename': X, 'class': y})\n",
    "        return data_gen.flow_from_dataframe(\n",
    "            dataframe=df,\n",
    "            x_col='filename',\n",
    "            y_col='class',\n",
    "            target_size=target_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode='raw',\n",
    "            shuffle=shuffle,\n",
    "            color_mode='rgb'\n",
    "        )\n",
    "    \n",
    "    train_gen = create_generator(train_datagen, X_train, y_train, shuffle=True)\n",
    "    val_gen = create_generator(val_test_datagen, X_val, y_val)\n",
    "    test_gen = create_generator(val_test_datagen, X_test, y_test)\n",
    "    \n",
    "    # Save class indices for reference\n",
    "    with open('class_indices.json', 'w') as f:\n",
    "        json.dump(class_indices, f)\n",
    "    \n",
    "    return train_gen, val_gen, test_gen, class_indices\n",
    "\n",
    "\n",
    "def preprocess_single_image(image_path, target_size=(512, 512)):\n",
    "    \"\"\"\n",
    "    Preprocess a single image for prediction.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        target_size (tuple): Target size for resizing\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed image as numpy array\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # Convert to RGB if not already\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    \n",
    "    # Resize\n",
    "    img = img.resize(target_size)\n",
    "    \n",
    "    # Convert to array and normalize\n",
    "    img_array = np.array(img) / 255.0\n",
    "    \n",
    "    # Add batch dimension\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "\n",
    "def patch_based_processing(image_path, patch_size=256, overlap=64):\n",
    "    \"\"\"\n",
    "    Process large images by dividing into patches.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        patch_size (int): Size of square patches\n",
    "        overlap (int): Overlap between patches\n",
    "        \n",
    "    Returns:\n",
    "        List of patches as numpy arrays\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    \n",
    "    width, height = img.size\n",
    "    patches = []\n",
    "    \n",
    "    # Calculate step size\n",
    "    step = patch_size - overlap\n",
    "    \n",
    "    # Extract patches\n",
    "    for y in range(0, height - overlap, step):\n",
    "        for x in range(0, width - overlap, step):\n",
    "            box = (x, y, x + patch_size, y + patch_size)\n",
    "            patch = img.crop(box)\n",
    "            patch_array = np.array(patch) / 255.0\n",
    "            patches.append(patch_array)\n",
    "    \n",
    "    return patches\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Example usage\n",
    "#     root_directory = \"/Users/sohail/Documents/Salmaan/ACPS_Lab/Project/dataset/RGB\"\n",
    "#     train_gen, val_gen, test_gen, class_indices = load_and_preprocess_images(root_directory)\n",
    "    \n",
    "#     print(f\"Found {len(class_indices)} classes: {class_indices}\")\n",
    "#     print(f\"Train batches: {len(train_gen)}\")\n",
    "#     print(f\"Validation batches: {len(val_gen)}\")\n",
    "#     print(f\"Test batches: {len(test_gen)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, callbacks\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 775 validated image filenames.\n",
      "Found 87 validated image filenames.\n",
      "Found 216 validated image filenames.\n",
      "Found 13 classes: {'no_adulteration': 0, 'Detergent_10%': 1, 'Shampoo_10%': 2, 'Water_10%': 3, 'StarchPowder_10%': 4, 'Detergent_15%': 5, 'Shampoo_15%': 6, 'Water_15%': 7, 'StarchPowder_15%': 8, 'Detergent_5%': 9, 'Shampoo_5%': 10, 'Water_5%': 11, 'StarchPowder_5%': 12}\n",
      "Train batches: 130\n",
      "Validation batches: 15\n",
      "Test batches: 36\n"
     ]
    }
   ],
   "source": [
    "root_directory = \"/home/nitin/salmaan/RGB\"\n",
    "train_gen, val_gen, test_gen, class_indices = load_and_preprocess_images(\n",
    "    root_directory,\n",
    "    target_size=(256, 256),  # Reduced from 512x512\n",
    "    batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "print(f\"Found {len(class_indices)} classes: {class_indices}\")\n",
    "print(f\"Train batches: {len(train_gen)}\")\n",
    "print(f\"Validation batches: {len(val_gen)}\")\n",
    "print(f\"Test batches: {len(test_gen)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes=13):\n",
    "    \"\"\"Create model inside the strategy scope\"\"\"\n",
    "    with strategy.scope():\n",
    "        # Use EfficientNetB2 as compromise between size and performance\n",
    "        model = models.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=(256,256,3)),\n",
    "        \n",
    "        # First convolution block\n",
    "        layers.Conv2D(32, (5, 5), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # Second convolution block\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Third convolution block\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Fourth convolution block\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        # Classifier head\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Custom learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "    callbacks.ModelCheckpoint('best_model_multi_gpu.h5', save_best_only=True),\n",
    "    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5),\n",
    "    callbacks.TerminateOnNaN(),\n",
    "    callbacks.BackupAndRestore('backup')  # Important for multi-GPU training\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 256, 256, 32)      2432      \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 256, 256, 32)      128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 128, 128, 32)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128, 128, 32)      0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 128, 128, 64)      18496     \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 128, 128, 64)      256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 64, 64, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64, 64, 64)        0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 64, 64, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 64, 64, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 32, 32, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 32, 32, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 32, 32, 256)       1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 16, 16, 256)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 65536)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               8388736   \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 13)                1677      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8782797 (33.50 MB)\n",
      "Trainable params: 8781581 (33.50 MB)\n",
      "Non-trainable params: 1216 (4.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nitin/miniforge3/envs/acps_salmaan/lib/python3.8/site-packages/PIL/Image.py:3368: DecompressionBombWarning: Image size (108576768 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "2025-04-04 08:14:37.832413: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_1\"\n",
      "op: \"TensorDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 1\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\017TensorDataset:0\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT32\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "INFO:tensorflow:Collective all_reduce tensors: 22 all_reduces, num_devices = 3, group_size = 3, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 22 all_reduces, num_devices = 3, group_size = 3, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 08:14:51.703093: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907\n",
      "2025-04-04 08:14:51.759369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907\n",
      "2025-04-04 08:14:51.777789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907\n",
      "2025-04-04 08:14:52.719807: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_2_bfc) ran out of memory trying to allocate 1.06GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-04-04 08:14:52.762181: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_1_bfc) ran out of memory trying to allocate 1.06GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-04-04 08:14:52.799803: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.06GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-04-04 08:14:52.807344: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_2_bfc) ran out of memory trying to allocate 1.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-04-04 08:14:52.826081: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_1_bfc) ran out of memory trying to allocate 1.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-04-04 08:14:52.842741: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-04-04 08:14:52.921256: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_2_bfc) ran out of memory trying to allocate 1.06GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-04-04 08:14:53.107445: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_1_bfc) ran out of memory trying to allocate 1.06GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-04-04 08:14:53.158901: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.06GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6/130 [>.............................] - ETA: 4:10 - loss: 3.6437 - accuracy: 0.0556"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 08:15:10.718919: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fcbd0005550 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-04-04 08:15:10.722764: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2025-04-04 08:15:10.722783: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2025-04-04 08:15:10.722793: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2025-04-04 08:15:10.970976: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-04-04 08:15:12.157637: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 50/130 [==========>...................] - ETA: 4:00 - loss: 3.2482 - accuracy: 0.0500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 08:17:24.970439: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.04GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - ETA: 0s - loss: 3.1019 - accuracy: 0.0632"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 08:21:08.239643: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_1\"\n",
      "op: \"TensorDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 1\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\020TensorDataset:35\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT32\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "/home/nitin/miniforge3/envs/acps_salmaan/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 431s 3s/step - loss: 3.1019 - accuracy: 0.0632 - val_loss: 6.4880 - val_accuracy: 0.0920 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "130/130 [==============================] - 381s 3s/step - loss: 2.9724 - accuracy: 0.0735 - val_loss: 4.4515 - val_accuracy: 0.0920 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "130/130 [==============================] - 385s 3s/step - loss: 2.7977 - accuracy: 0.0903 - val_loss: 6.5385 - val_accuracy: 0.1264 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "130/130 [==============================] - 394s 3s/step - loss: 2.7649 - accuracy: 0.0839 - val_loss: 4.9956 - val_accuracy: 0.1034 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "130/130 [==============================] - 380s 3s/step - loss: 2.7000 - accuracy: 0.0994 - val_loss: 3.3951 - val_accuracy: 0.1494 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = create_model(num_classes=len(class_indices))\n",
    "    model.summary()\n",
    "\n",
    "    # Training\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        epochs=5,\n",
    "        validation_data=val_gen,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 08:47:41.799326: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_1\"\n",
      "op: \"TensorDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 1\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021TensorDataset:222\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT32\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 107s 3s/step - loss: 3.1328 - accuracy: 0.0926\n",
      "Test Accuracy: 0.0926\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_loss, test_acc = model.evaluate(test_gen)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Save class indices and results\n",
    "with open('class_indices.json', 'w') as f:\n",
    "    json.dump(class_indices, f)\n",
    "\n",
    "with open('training_results.txt', 'w') as f:\n",
    "    f.write(f\"Test Accuracy: {test_acc:.4f}\\n\")\n",
    "    f.write(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acps_salmaan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
